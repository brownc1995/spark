{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "from pyspark import SparkContext, SparkFiles, StorageLevel\n",
    "from operator import add\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.serializers import MarshalSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(\"/Users/owner/spark\")\n",
    "work_dir = '/Users/owner/Documents/git/spark/examples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs\n",
    "RDD stands for Resilient Distributed Dataset. These are the elements that run and operate on multiple nodes to do parallel processing on a cluster. RDDs are immutable and are fault tolerant as well i.e. in case of failure they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.\n",
    "\n",
    "There are two ways to apply operations on RDDs:\n",
    "1. __Transformation__: These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "2. __Action__: These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(appName=\"examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "Number of elements in the RDD is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "counts = words.count()\n",
    "print(f'Number of elements in RDD -> {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect\n",
    "All the elements in the RDD are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "coll = words.collect()\n",
    "print(f'Elements in RDD -> {coll}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "A new RDD is returned containing the elements, which satisfies the function inside the filter. In the following example, we filter out the strings containing ''spark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(f'Filtered RDD -> {filtered}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "A new RDD is returned by applying a function to each element in the RDD. In the following example, we form a key value pair and map every string with a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(f'Key value pair -> {mapping}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY). You can also check if the RDD is cached or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached -> True\n"
     ]
    }
   ],
   "source": [
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(f'Words got chached -> {caching}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce\n",
    "After performing the specified commutative and associative binary operation, the element in the RDD is returned. In the following example, we are importing add package from the operator and applying it on ‘num’ to carry out a simple addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Reduce_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "adding = nums.reduce(add)\n",
    "print(f'Adding all the elements -> {adding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "It returns RDD with a pair of elements with the matching keys and all the values for that particular key. In the following example, there are two pair of elements in two different RDDs. After joining these two RDDs, we get an RDD with elements having matching keys and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Join_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(f'Join RDD -> {final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing\n",
    "For parallel processing, Apache Spark uses shared variables. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks.\n",
    "\n",
    "There are two types of shared variables supported by Apache Spark: __Broadcast__ and __Accumulator__.\n",
    "\n",
    "## Broadcast\n",
    "Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Broadcast_app\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = sc.broadcast(\n",
    "    [\"scala\", \n",
    "     \"java\", \n",
    "     \"hadoop\", \n",
    "     \"spark\", \n",
    "     \"akka\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored data -> ['scala', 'java', 'hadoop', 'spark', 'akka']\n",
      "Printing a particular element in RDD -> hadoop\n"
     ]
    }
   ],
   "source": [
    "data = words_new.value \n",
    "elem = words_new.value[2] \n",
    "print(f'Stored data -> {data}')\n",
    "print(f'Printing a particular element in RDD -> {elem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n",
    "Accumulator variables are used for aggregating the information through associative and commutative operations. For example, you can use an accumulator for a sum operation or counters (in MapReduce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Accumulator app\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = sc.accumulator(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated value is -> 150\n"
     ]
    }
   ],
   "source": [
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print(f'Accumulated value is -> {final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files\n",
    "In Apache Spark, you can upload your files using `sc.addFile` and get the path on a worker using `SparkFiles.get`. Thus, SparkFiles resolve the paths to files added through SparkContext.addFile().\n",
    "\n",
    "SparkFiles contain the following classmethods\n",
    "1. `get(filename)`\n",
    "2. `getrootdirectory()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"SparkFile_App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Path -> /private/var/folders/0n/z4c7t4dd59zdjk155nrygs2r0000gn/T/spark-9e39d9ab-c9c3-4376-a5ca-2a696ad27497/userFiles-65b6676d-a1e7-4c35-8b8b-fe020185a9c0/intro_to_spark.ipynb\n"
     ]
    }
   ],
   "source": [
    "file_to_add='intro_to_spark.ipynb'\n",
    "testfile = f'{work_dir}/{file_to_add}'\n",
    "sc.addFile(testfile)\n",
    "print(f'Absolute Path -> {SparkFiles.get(file_to_add)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to http://localhost:4040/environment/ then you'll be able to see your added file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StorageLevel\n",
    "`StorageLevel` specifies how RDDs should be stored: in memory, disk, or both. It also specifies whether to serialize RDDs and whether to replicate RDD partitions.\n",
    "\n",
    "The following code block has the class definition of a `StorageLevel`:\n",
    "\n",
    "`class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)`\n",
    "\n",
    "Now, to decide the storage of RDD, there are different storage levels, which are given below:\n",
    "\n",
    "- __DISK_ONLY__: `StorageLevel(True, False, False, False, 1)`\n",
    "- __DISK_ONLY_2__: `StorageLevel(True, False, False, False, 2)`\n",
    "- __MEMORY_AND_DISK__: `StorageLevel(True, True, False, False, 1)`\n",
    "- __MEMORY_AND_DISK_2__: `StorageLevel(True, True, False, False, 2)`\n",
    "- __MEMORY_AND_DISK_SER__: `StorageLevel(True, True, False, False, 1)`\n",
    "- __MEMORY_AND_DISK_SER_2__: `StorageLevel(True, True, False, False, 2)`\n",
    "- __MEMORY_ONLY__: `StorageLevel(False, True, False, False, 1)`\n",
    "- __MEMORY_ONLY_2__: `StorageLevel(False, True, False, False, 2)`\n",
    "- __MEMORY_ONLY_SER__: `StorageLevel(False, True, False, False, 1)`\n",
    "- __MEMORY_ONLY_SER_2__: `StorageLevel(False, True, False, False, 2)`\n",
    "- __OFF_HEAP__: `StorageLevel(True, True, True, False, 1)`\n",
    "\n",
    "Let us consider the following example of StorageLevel, where we use the storage level __MEMORY_AND_DISK_2__, which means RDD partitions will have replication of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext (\n",
    "   \"local\", \n",
    "   \"storagelevel app\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "rdd1.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "rdd1.getStorageLevel()\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib\n",
    "Apache Spark offers a Machine Learning API called MLlib. PySpark has this machine learning API in Python as well. Here are some of its offerings:\n",
    "- __mllib.classification__: The spark.mllib package supports various methods for binary classification, multiclass classification and regression analysis. Some of the most popular algorithms in classification are Random Forest, Naive Bayes, Decision Tree, etc.\n",
    "- __mllib.clustering__: Clustering is an unsupervised learning problem, whereby you aim to group subsets of entities with one another based on some notion of similarity.\n",
    "- __mllib.fpm__: Frequent pattern matching is mining frequent items, itemsets, subsequences or other substructures that are usually among the first steps to analyze a large-scale dataset. This has been an active research topic in data mining for years.\n",
    "- __mllib.linalg__: MLlib utilities for linear algebra.\n",
    "- __mllib.recommendation__: Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user item association matrix.\n",
    "- __spark.mllib__: It currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.mllib uses the Alternating Least Squares (ALS) algorithm to learn these latent factors.\n",
    "- __mllib.regression__: Linear regression belongs to the family of regression algorithms. The goal of regression is to find relationships and dependencies between variables. The interface for working with linear regression models and model summaries is similar to the logistic regression case.\n",
    "\n",
    "There are other algorithms, classes and functions also as a part of the mllib package. As of now, let us understand a demonstration on pyspark.mllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(appName=\"Pyspark_mllib_Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 8.338085482353445e-06\n"
     ]
    }
   ],
   "source": [
    "data = sc.textFile('test_data.txt')\n",
    "ratings = data.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(f'Mean Squared Error = {MSE}')\n",
    "\n",
    "# Save and load model\n",
    "if not os.path.isdir(f'{work_dir}/myCollaborativeFilter'):\n",
    "    model.save(sc, f'{work_dir}/myCollaborativeFilter')\n",
    "sameModel = MatrixFactorizationModel.load(sc, f'{work_dir}/myCollaborativeFilter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializers\n",
    "Serialization is used for performance tuning on Apache Spark. All data that is sent over the network or written to disk or persisted in the memory should be serialized for improved performance. Serialization plays an important role in costly operations.\n",
    "\n",
    "PySpark supports custom serializers for performance tuning. The following two serializers are supported by PySpark:\n",
    "- __MarshalSerializer__: Serializes objects using Python’s Marshal Serializer. This serializer is faster than PickleSerializer, but supports fewer datatypes.\n",
    "- __PickleSerializer__: Serializes objects using Python’s Pickle Serializer. This serializer supports nearly any Python object, but may not be as fast as more specialized serializers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \n",
    "                  \"serialization app\", \n",
    "                  serializer = MarshalSerializer()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize(list(range(1000))).map(lambda x: 2 * x).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
